{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1189&EId=3\" #ensure that ur network doesnt block this\n",
    "#params below is for the file naming convention\n",
    "eventname = \"ThistlePDOWSClassic\"\n",
    "startdate = \"20240203\"\n",
    "category = \"2k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A class for scraping data from web pages.\n",
    "\n",
    "    Attributes:\n",
    "        start_url (str): The URL of the web page to scrape.\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML content.\n",
    "        tables (list): A list of BeautifulSoup Tag objects representing the tables in the web page.\n",
    "\n",
    "    Methods:\n",
    "        fetch_data(): Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        get_data_table(): Retrieves the data table from the web page.\n",
    "        get_page_list(): Retrieves the list of URLs from the web page.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_url):\n",
    "        self.start_url = start_url\n",
    "        self.base_url = \"\"\n",
    "        # Initialize variables\n",
    "        self.soup = None\n",
    "        self.tables = []\n",
    "        self.data = None  # This will store the dataframe\n",
    "        self.other_pages_url = []\n",
    "        self.get_base_url()\n",
    "        self.fetch_data()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"\n",
    "        Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.start_url)\n",
    "            response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "            self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            self.tables = self.soup.find_all(\"table\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "\n",
    "    def get_base_url(self):\n",
    "        \"\"\"\n",
    "        Removes the strings after \".com/\" in a given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The URL with the strings after \".com/\" removed.\n",
    "        \"\"\"\n",
    "        index = self.start_url.find(\".com/\")\n",
    "        if index != -1:\n",
    "            self.base_url = self.start_url[:index + 5]\n",
    "        else:\n",
    "            self.base_url = self.start_url\n",
    "\n",
    "\n",
    "    def get_data_table(self):\n",
    "        \"\"\"\n",
    "        Retrieves the data table from the web page.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The data table as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        # Assuming the first table is the data table\n",
    "        if len(self.tables) < 2:\n",
    "            print(\"Data table not found.\")\n",
    "            return None\n",
    "        rows = self.tables[1].find_all(\"tr\")\n",
    "        rows_list = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            cols_list = [col.text for col in cols if \"d-sm-none\" not in col.get(\"class\", [])]\n",
    "            if cols_list:  # Ensure the list is not empty\n",
    "                rows_list.append(cols_list)\n",
    "        if not rows_list:\n",
    "            print(\"No data found in the table.\")\n",
    "            return None\n",
    "        df = pd.DataFrame(rows_list)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:, :]\n",
    "        return df\n",
    "\n",
    "    def get_page_list(self):\n",
    "        \"\"\"\n",
    "        Retrieves the list of URLs from the web page.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of URLs.\n",
    "        \"\"\"\n",
    "        # Assuming the first table contains links to other pages\n",
    "        if not self.tables:\n",
    "            print(\"No tables found.\")\n",
    "            return None\n",
    "        links = [a['href'] for a in self.tables[0].find_all(\"a\", href=True)]\n",
    "        if not links:\n",
    "            print(\"No URLs found.\")\n",
    "            return None\n",
    "        # You might want to handle relative URLs here   \n",
    "        links_updated = []\n",
    "        for link in links:\n",
    "            links_updated.append(self.base_url + link)\n",
    "\n",
    "\n",
    "        self.other_pages_url = links_updated\n",
    "        return links_updated\n",
    "\n",
    "class PageIterator(WebScraper):\n",
    "    \"\"\"\n",
    "    A class that iterates through a list of page URLs and fetches data tables from each page.\n",
    "\n",
    "    Attributes:\n",
    "    - page_list (list): A list of page URLs to iterate through.\n",
    "    - current_page (int): The index of the current page being processed.\n",
    "    - tables (list): A list to store all tables from all pages.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, urls): Initializes the PageIterator object with a list of page URLs.\n",
    "    - fetch_all_pages(self): Fetches data tables from all pages in the page_list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, urls):\n",
    "        self.page_list = urls\n",
    "        self.current_page = 0\n",
    "        self.tables = []  # Store all tables from all pages\n",
    "        self.fetch_all_pages()\n",
    "\n",
    "    def fetch_all_pages(self):\n",
    "        \"\"\"\n",
    "        Fetches data tables from all pages in the page_list.\n",
    "\n",
    "        Returns:\n",
    "        - tables (list): A list of data tables fetched from all pages.\n",
    "        \"\"\"\n",
    "        for page_url in self.page_list:\n",
    "            self.web_scraper = WebScraper(page_url)\n",
    "            table = self.web_scraper.get_data_table()\n",
    "            if table is not None:\n",
    "                self.tables.append(table)\n",
    "\n",
    "        if len(self.tables) == 0:\n",
    "            print(\"No tables found.\")\n",
    "\n",
    "        return self.tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Race No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Category</th>\n",
       "      <th>Cat Pos</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Gen Pos</th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>521</td>\n",
       "      <td>THONG ZHEN NING</td>\n",
       "      <td>00:25:25</td>\n",
       "      <td>16-19</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Wahoo Swimming Teamn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>373</td>\n",
       "      <td>LEONG WEI SHENG IAN</td>\n",
       "      <td>00:25:36</td>\n",
       "      <td>16-19</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>Singapore Sports School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>309</td>\n",
       "      <td>GABRIEL FRANCIS MCCARTAN</td>\n",
       "      <td>00:25:43</td>\n",
       "      <td>16-19</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>DSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>219</td>\n",
       "      <td>AIMAN HAKIMI BIN AHMAD SABAN</td>\n",
       "      <td>00:27:43</td>\n",
       "      <td>16-19</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>Kujira Swim Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>346</td>\n",
       "      <td>KAM YAO QI SAMUEL</td>\n",
       "      <td>00:28:11</td>\n",
       "      <td>16-19</td>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>5</td>\n",
       "      <td>Team Dugong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>312</td>\n",
       "      <td>481</td>\n",
       "      <td>SAW PEIH CHING</td>\n",
       "      <td>01:20:47</td>\n",
       "      <td>35-39</td>\n",
       "      <td>12</td>\n",
       "      <td>Female</td>\n",
       "      <td>87</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>313</td>\n",
       "      <td>429</td>\n",
       "      <td>MUHAMMAD EMIR IKHMAL BIN ISMAIL</td>\n",
       "      <td>01:22:05</td>\n",
       "      <td>20-24</td>\n",
       "      <td>8</td>\n",
       "      <td>Male</td>\n",
       "      <td>226</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>314</td>\n",
       "      <td>529</td>\n",
       "      <td>WONG CHEE HOO</td>\n",
       "      <td>01:22:48</td>\n",
       "      <td>40-44</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>227</td>\n",
       "      <td>Dugong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>315</td>\n",
       "      <td>261</td>\n",
       "      <td>CHAN YI JIUN</td>\n",
       "      <td>01:23:13</td>\n",
       "      <td>40-44</td>\n",
       "      <td>17</td>\n",
       "      <td>Female</td>\n",
       "      <td>88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>316</td>\n",
       "      <td>367</td>\n",
       "      <td>LEE SOK YIN</td>\n",
       "      <td>01:23:20</td>\n",
       "      <td>50-54</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "      <td>89</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   Pos Race No                              Name      Time Category Cat Pos  \\\n",
       "1     1     521                  THONG ZHEN NING   00:25:25    16-19       1   \n",
       "2     2     373              LEONG WEI SHENG IAN   00:25:36    16-19       2   \n",
       "3     3     309        GABRIEL FRANCIS MCCARTAN    00:25:43    16-19       3   \n",
       "4     4     219     AIMAN HAKIMI BIN AHMAD SABAN   00:27:43    16-19       4   \n",
       "5     5     346                KAM YAO QI SAMUEL   00:28:11    16-19       5   \n",
       "..  ...     ...                               ...       ...      ...     ...   \n",
       "12  312     481                   SAW PEIH CHING   01:20:47    35-39      12   \n",
       "13  313     429  MUHAMMAD EMIR IKHMAL BIN ISMAIL   01:22:05    20-24       8   \n",
       "14  314     529                    WONG CHEE HOO   01:22:48    40-44      31   \n",
       "15  315     261                     CHAN YI JIUN   01:23:13    40-44      17   \n",
       "16  316     367                      LEE SOK YIN   01:23:20    50-54       8   \n",
       "\n",
       "0   Gender Gen Pos                     Team  \n",
       "1     Male       1     Wahoo Swimming Teamn  \n",
       "2     Male       2  Singapore Sports School  \n",
       "3     Male       3                      DSA  \n",
       "4     Male       4         Kujira Swim Club  \n",
       "5     Male       5              Team Dugong  \n",
       "..     ...     ...                      ...  \n",
       "12  Female      87                           \n",
       "13    Male     226                           \n",
       "14    Male     227                   Dugong  \n",
       "15  Female      88                           \n",
       "16  Female      89                           \n",
       "\n",
       "[316 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a WebScraper object with the start URL\n",
    "km1 = WebScraper(start_url=url)\n",
    "\n",
    "# Get the data table from the start URL\n",
    "data = km1.get_data_table()\n",
    "\n",
    "# Get the list of other pages to scrape\n",
    "page_list = km1.get_page_list()\n",
    "\n",
    "# Create a PageIterator object with the list of pages\n",
    "other_data = PageIterator(page_list)\n",
    "\n",
    "# Get the tables from the other pages\n",
    "all_data = other_data.tables\n",
    "\n",
    "# Append the data table from the start URL to the list of all data tables\n",
    "all_data.append(data)\n",
    "\n",
    "# Concatenate all data tables into a single DataFrame\n",
    "df_all = pd.concat(all_data)\n",
    "\n",
    "# Remove rows where the 'Pos' column is empty\n",
    "df_all = df_all[df_all['Pos'] != '']\n",
    "\n",
    "# Convert the 'Pos' column to integer type\n",
    "df_all['Pos'] = df_all['Pos'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by the 'Pos' column\n",
    "df_all = df_all.sort_values(by=['Pos'])\n",
    "\n",
    "# Remove rows where the 'Time' column does not contain a colon (i.e., is not in HH:MM:SS format)\n",
    "df_all = df_all[df_all['Time'].str.contains(':')]\n",
    "\n",
    "# Remove leading and trailing whitespace from the 'Team' column\n",
    "df_all[\"Team\"] = df_all[\"Team\"].str.strip()\n",
    "\n",
    "# Replace \"N/A\" with an empty string in the 'Team' column\n",
    "df_all[\"Team\"] = df_all[\"Team\"].str.replace(\"N/A\", \"\")\n",
    "\n",
    "#Remove uneccessary Column\n",
    "df_all = df_all.drop('Fav', axis=1)\n",
    "df_all \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file to csv. the format should be [eventname]_[startdate]_[category].csv\n",
    "\n",
    "df_all.to_csv(f\"../Dataset/{eventname}_{startdate}_{category}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
