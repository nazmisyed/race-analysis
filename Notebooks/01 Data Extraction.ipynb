{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A class for scraping data from web pages.\n",
    "\n",
    "    Attributes:\n",
    "        start_url (str): The URL of the web page to scrape.\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML content.\n",
    "        tables (list): A list of BeautifulSoup Tag objects representing the tables in the web page.\n",
    "\n",
    "    Methods:\n",
    "        fetch_data(): Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        get_data_table(): Retrieves the data table from the web page.\n",
    "        get_page_list(): Retrieves the list of URLs from the web page.\n",
    "        iterate_over_pages(): Iterates over each URL in the list and processes the data table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_url):\n",
    "        self.start_url = start_url\n",
    "        self.base_url = \"\"\n",
    "        self.soup = None\n",
    "        self.tables = []\n",
    "        self.data = None #this will store dataframe\n",
    "        self.get_base_url()\n",
    "        self.fetch_data()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"\n",
    "        Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.start_url)\n",
    "            response.raise_for_status() # Raise an exception if the request was unsuccessful\n",
    "            self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            self.tables = self.soup.find_all(\"table\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "\n",
    "    def get_base_url(self):\n",
    "        \"\"\"\n",
    "        Removes the strings after \".com/\" in a given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The URL with the strings after \".com/\" removed.\n",
    "        \"\"\"\n",
    "        index = self.url.find(\".com/\")\n",
    "        if index != -1:\n",
    "            self.base_url =  self.url[:index + 5]\n",
    "        self.base_url = url\n",
    "\n",
    "\n",
    "    def get_data_table(self):\n",
    "        \"\"\"\n",
    "        Retrieves the data table from the web page.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The data table as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        # Assuming the first table is the data table\n",
    "        if len(self.tables) < 2:\n",
    "            print(\"Data table not found.\")\n",
    "            return None\n",
    "        rows = self.tables[1].find_all(\"tr\")\n",
    "        rows_list = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            cols_list = [col.text for col in cols if \"d-sm-none\" not in col.get(\"class\", [])]\n",
    "            if cols_list: # Ensure the list is not empty\n",
    "                rows_list.append(cols_list)\n",
    "        if not rows_list:\n",
    "            print(\"No data found in the table.\")\n",
    "            return None\n",
    "        df = pd.DataFrame(rows_list)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:, :]\n",
    "        return df\n",
    "\n",
    "    def get_page_list(self):\n",
    "        \"\"\"\n",
    "        Retrieves the list of URLs from the web page.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of URLs.\n",
    "        \"\"\"\n",
    "        # Assuming the first table contains links to other pages\n",
    "        if not self.tables:\n",
    "            print(\"No tables found.\")\n",
    "            return None\n",
    "        links = [a['href'] for a in self.tables[0].find_all(\"a\", href=True)]\n",
    "        return links\n",
    "\n",
    "    # def iterate_over_pages(self):\n",
    "    #     \"\"\"\n",
    "    #     Iterates over each URL in the list and processes the data table.\n",
    "    #     \"\"\"\n",
    "    #     # Get the list of URLs from the first page\n",
    "    #     urls = self.get_page_list()\n",
    "    #     if urls is None:\n",
    "    #         print(\"No URLs found for iteration.\")\n",
    "    #         return\n",
    "    #     # Iterate over each URL and process it\n",
    "    #     for url in urls:\n",
    "    #         # You might want to handle relative URLs here\n",
    "    #         self.start_url = url\n",
    "    #         self.fetch_data()\n",
    "    #         data_table = self.get_data_table()\n",
    "    #         if data_table is not None:\n",
    "    #             # Process the data table as needed\n",
    "    #             print(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1189&EId=2\"\n",
    "km1 = WebScraper(start_url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results.aspx?CId=16634&RId=1189&EId=2&dt=0&PageNo=2',\n",
       " 'results.aspx?CId=16634&RId=1189&EId=2&dt=0&PageNo=3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data   = km1.get_data_table()\n",
    "page_list = km1.get_page_list()\n",
    "\n",
    "page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://runnersunite.racetecresults.com/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_string = remove_strings_after_com(url)\n",
    "new_string\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
