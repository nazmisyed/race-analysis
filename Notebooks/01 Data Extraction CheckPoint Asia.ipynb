{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://results.checkpointspot.asia/results.aspx?CId=17036&RId=10510&EId=40\" #ensure that ur network doesnt block this\n",
    "#params below is for the file naming convention\n",
    "eventname = \"Asia Triathlon Cup 2025\"\n",
    "startdate = \"20250222\"\n",
    "category = \"SprintAquathlon5059F\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A class for scraping data from web pages.\n",
    "\n",
    "    Attributes:\n",
    "        start_url (str): The URL of the web page to scrape.\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML content.\n",
    "        tables (list): A list of BeautifulSoup Tag objects representing the tables in the web page.\n",
    "\n",
    "    Methods:\n",
    "        fetch_data(): Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        get_data_table(): Retrieves the data table from the web page.\n",
    "        get_page_list(): Retrieves the list of URLs from the web page.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_url):\n",
    "        self.start_url = start_url\n",
    "        self.base_url = \"\"\n",
    "        # Initialize variables\n",
    "        self.soup = None\n",
    "        self.tables = []\n",
    "        self.data = None  # This will store the dataframe\n",
    "        self.other_pages_url = []\n",
    "        self.get_base_url()\n",
    "        self.fetch_data()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"\n",
    "        Fetches the HTML content of the web page and parses it using BeautifulSoup.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.start_url)\n",
    "            response.raise_for_status()  # Raise an exception if the request was unsuccessful\n",
    "            self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            self.tables = self.soup.find_all(\"table\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "\n",
    "    def get_base_url(self):\n",
    "        \"\"\"\n",
    "        Removes the strings after \".com/\" in a given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The URL with the strings after \".com/\" removed.\n",
    "        \"\"\"\n",
    "        index = self.start_url.find(\".com/\")\n",
    "        if index != -1:\n",
    "            self.base_url = self.start_url[:index + 5]\n",
    "        else:\n",
    "            self.base_url = self.start_url\n",
    "\n",
    "\n",
    "    def get_data_table(self):\n",
    "        \"\"\"\n",
    "        Retrieves the data table from the web page.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The data table as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        # Assuming the first table is the data table\n",
    "        if len(self.tables) < 2:\n",
    "            print(\"Data table not found.\")\n",
    "            return None\n",
    "        rows = self.tables[1].find_all(\"tr\")\n",
    "        rows_list = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            cols_list = [col.text for col in cols if \"d-sm-none\" not in col.get(\"class\", [])]\n",
    "            if cols_list:  # Ensure the list is not empty\n",
    "                rows_list.append(cols_list)\n",
    "        if not rows_list:\n",
    "            print(\"No data found in the table.\")\n",
    "            return None\n",
    "        df = pd.DataFrame(rows_list)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:, :]\n",
    "        return df\n",
    "\n",
    "    def get_page_list(self):\n",
    "        \"\"\"\n",
    "        Retrieves the list of URLs from the web page.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of URLs.\n",
    "        \"\"\"\n",
    "        # Assuming the first table contains links to other pages\n",
    "        if not self.tables:\n",
    "            print(\"No tables found.\")\n",
    "            return None\n",
    "        links = [a['href'] for a in self.tables[0].find_all(\"a\", href=True)]\n",
    "        if not links:\n",
    "            print(\"No URLs found.\")\n",
    "            return None\n",
    "        # You might want to handle relative URLs here   \n",
    "        links_updated = []\n",
    "        for link in links:\n",
    "            links_updated.append(self.base_url + link)\n",
    "\n",
    "\n",
    "        self.other_pages_url = links_updated\n",
    "        return links_updated\n",
    "\n",
    "class PageIterator(WebScraper):\n",
    "    \"\"\"\n",
    "    A class that iterates through a list of page URLs and fetches data tables from each page.\n",
    "\n",
    "    Attributes:\n",
    "    - page_list (list): A list of page URLs to iterate through.\n",
    "    - current_page (int): The index of the current page being processed.\n",
    "    - tables (list): A list to store all tables from all pages.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, urls): Initializes the PageIterator object with a list of page URLs.\n",
    "    - fetch_all_pages(self): Fetches data tables from all pages in the page_list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, urls):\n",
    "        self.page_list = urls\n",
    "        self.current_page = 0\n",
    "        self.tables = []  # Store all tables from all pages\n",
    "        self.fetch_all_pages()\n",
    "\n",
    "    def fetch_all_pages(self):\n",
    "        \"\"\"\n",
    "        Fetches data tables from all pages in the page_list.\n",
    "\n",
    "        Returns:\n",
    "        - tables (list): A list of data tables fetched from all pages.\n",
    "        \"\"\"\n",
    "        for page_url in self.page_list:\n",
    "            self.web_scraper = WebScraper(page_url)\n",
    "            table = self.web_scraper.get_data_table()\n",
    "            if table is not None:\n",
    "                self.tables.append(table)\n",
    "\n",
    "        if len(self.tables) == 0:\n",
    "            print(\"No tables found.\")\n",
    "\n",
    "        return self.tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No URLs found.\n",
      "No other pages to scrape.\n"
     ]
    }
   ],
   "source": [
    "# Create a WebScraper object with the start URL\n",
    "km1 = WebScraper(start_url=url)\n",
    "\n",
    "# Get the data table from the start URL\n",
    "data = km1.get_data_table()\n",
    "\n",
    "# Get the list of other pages to scrape\n",
    "page_list = km1.get_page_list()\n",
    "\n",
    "if page_list is None:\n",
    "    print(\"No other pages to scrape.\")\n",
    "    df_all = data\n",
    "    \n",
    "else:\n",
    "    # Create a PageIterator object with the list of pages\n",
    "    other_data = PageIterator(page_list)\n",
    "\n",
    "    # Get the tables from the other pages\n",
    "    all_data = other_data.tables\n",
    "\n",
    "    # Append the data table from the start URL to the list of all data tables\n",
    "    all_data.append(data)\n",
    "\n",
    "    # Concatenate all data tables into a single DataFrame\n",
    "    df_all = pd.concat(all_data)\n",
    "\n",
    "# Remove rows where the 'Pos' column is empty\n",
    "df_all = df_all[df_all['Pos'] != '']\n",
    "\n",
    "# Remove rows where the 'Pos' column is NaN\n",
    "df_all = df_all.dropna(subset=['Pos'])\n",
    "\n",
    "# Convert the 'Pos' column to integer type\n",
    "df_all['Pos'] = df_all['Pos'].astype(int)\n",
    "\n",
    "# Sort the DataFrame by the 'Pos' column\n",
    "df_all = df_all.sort_values(by=['Pos'])\n",
    "\n",
    "# Remove rows where the 'Time' column does not contain a colon (i.e., is not in HH:MM:SS format)\n",
    "df_all = df_all[df_all['Time'].str.contains(':')]\n",
    "\n",
    "df_all[\"Category\"] = category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file to csv. the format should be [eventname]_[startdate]_[category].csv\n",
    "\n",
    "df_all.to_csv(f\"../Dataset/{eventname}_{startdate}_{category}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
